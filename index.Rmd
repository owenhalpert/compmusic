---
title: "ðŸŒ´ Coachella Over The Years"
author: "Owen Halpert"
#date: "2023-02-14"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: rows
    theme: lumen
    css: custom.css
    favicon: favicon.ico
---

```{r setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(spotifyr)
library(ggplot2)
library(ggthemes)
library(tidyverse)
library(plotly)
library(compmus)
coachella2023 <- readRDS(file = "data/coachella2023.RDS")
coachella2022 <- readRDS(file = "data/coachella2022.RDS")
```

# Introduction

#### An exploration of the genres, artists, and specific audio features that make up one of the most popular music festivals in the world.

I owe a lot of my passion for music and my broad taste in genres to attending and following Coachella over the years. With an extremely diverse lineup, attendees can walk from one stage to the next and instantly be transported between wildly different musical experiences.

Even with this diversity, every year there are critics of the released Coachella lineup, as seen through the Reddit lineup announcement comments:

> Yea....this isn't the festival I went to in 2010ðŸ˜” -bigbrett666

> Awesome if you like pop and edm but sucks if you like live music. -dpcpv75

> This Coachella lineup is MID. No catering to the hip hop and rap fans. SMH debating selling my box -kmart2588

Some people say too much electronic, some say too little. Some say the artists are too new and unrecognizable, some complain about too many throwback artists and want more relevance.

While there is always some validity in each of these criticisms, for my project I'd like to further analyze the trends of Coachella artist selection. To do so, I will use musicological metrics to show variance in genre, danceability over the years, and changing levels of emotional "valence" (A Spotify measure from 0.0 to 1.0 describing the "musical positiveness" conveyed by a track). I will look at artists' live performances at Coachella, analyze melodic aspects of different tracks, and more.

The Spotify tracks I will use to analyze this progression will be yearly lineup playlists. I can then analyze and compare years as a whole. These playlists were assembled by user [Alex Rodriguez](https://open.spotify.com/user/aarod333){target="_blank"}, who explained that the playlists are made up of "Top 5 songs per artist, attempt to spread across their notable releases and shorter edits where possible."

Shortcomings with this methodology might be that the selected tracks for each artist may not be representative of their true genre, that the playlists may not always be fully inclusive of all artists on the lineup, or that they are simply so large that meaningful comparisons by some metrics will be difficult to make. Nonetheless, we can use these as a starting point to focus in on some trends within instead of trying to analyze the years as a whole.

To give a sense of the breadth of these playlists, we can examine the outliers. Looking at the [2023 lineup playlist](https://open.spotify.com/playlist/2HfbF1Xx4RHhJI8jNvQAQb){target="_blank"}, an extremely atypical track is BTSTU by Jai Paul. Paul was added to the lineup as an artist with a cult following who is extremely secretive and has never performed live. He is as elusive, if not more, than headliner Frank Ocean. Conversely, we see One Kiss by Calvin Harris, which we can say is much more typical, given that Harris has performed at Coachella 8 times and is one of the most mainstream EDM artists.

While these playlists will serve as a guide, most of the actual analysis will be done on individual tracks or segments of these playlists rather than analyzing them as a whole.

# Recent Year Overviews {data-orientation="columns"}

## Column {data-width="200"}

### **Commentary**

The below charts give a high-level broad overview of major audio features for tracks in the [2022](https://open.spotify.com/playlist/2tcwxeUSNT7gYMzidaf5W1){target="_blank"} and [2023](https://open.spotify.com/playlist/2HfbF1Xx4RHhJI8jNvQAQb){target="_blank"} playlists. Specifically, captured are the valence, energy, danceability (size of dots, mouse over for values) and mode. With the size of these datasets it is difficult to make broad comparisons, but some observations could be as follows:

â€¢ While 2022 shows most of the high valence songs having a major key, 2023 is more of a mix, with some of the highest valence songs being in a minor key.

â€¢ There are slightly fewer high valence, low energy tracks in 2022 than there are in 2023.

â€¢ Similar to the first point, upon isolating the minor key tracks (this can be done by clicking the legend) they seem to be on average higher energy and valence than the minor key tracks in 2022.

In the next page, we will further examine danceability.

```{r mutate, echo=FALSE}
coachella20232 <- coachella2023 %>%               
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  )
coachella20222 <- coachella2022 %>%               
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  )
```

## Column {.tabset data-width="800"}

### **2022**

```{r graph1, echo=FALSE}
plot1 <- ggplot(coachella20222, 
                aes(x = valence,
                    y = energy,
                    color = mode,
                    size = danceability)) +
  geom_point(position = "jitter", alpha = 0.4) +
  labs(x = "Valence", y = "Energy", color = "Mode", size = "", title = "Coachella 2022") +
  theme(legend.position="bottom") +
  theme_minimal()

ggplotly(plot1)
```

### **2023**

```{r graph2}
plot2 <- ggplot(coachella20232, aes(x = valence, y = energy, size = danceability, color = mode)) +
  geom_point(position = "jitter", alpha = 0.4) +
  labs(x = "Valence", y = "Energy", color = "Mode", size = "", title = "Coachella 2023") +
  theme_minimal()
  
ggplotly(plot2)
```

# Deeper into Danceability

### **Commentary** {data-width="200"}

Over the past seven years, with the rise of EDM in pop culture and in Coachella's lineups, one may ask if the festival is becoming more dancey. What we see below is the average tempi and danceability scores (a Spotify quantification) from each year. This is no easy task computationally speaking, in fact, the graph below involves over 6,000 data points, around 1000 songs curated from each year's lineup by [Alex Rodriguez](https://open.spotify.com/user/aarod333?si=271917dd12624d62){target="_blank"}.

We can see the average tempo mostly decreasing, and the average danceability mostly increasing. This is interesting, because normally you would assume these metrics go hand in hand. In reality, Spotify doesn't explicitly give their calculation methodology for Danceability. They simply say,

"Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable."

So, if Danceability were to increase with tempo decreasing, maybe there has been a rise in these other categories of rhythm stability, beat strength, and overall regularity. If you look at [this graphic](https://www.billboard.com/wp-content/uploads/media/history-of-coachella-acts-by-genre-2015-billboard-graphic-510.jpg){target="_blank"} by Billboard, you can see a sharp increase in house/dance music and steady prevalence of electronic, which may explain the uptick in danceability.

### **Danceability** {data-width="800"}

```{r}
sixteen <- 
  readRDS(file = "data/sixteen.RDS")

seventeen <- 
  readRDS(file = "data/seventeen.RDS")

eighteen <- 
  readRDS(file = "data/eighteen.RDS")

nineteen <- 
  readRDS(file = "data/nineteen.RDS")

twentytwo <- 
  readRDS(file = "data/twentytwo.RDS")

twentythree <-
  readRDS(file = "data/twentythree.RDS")

ssixteen <- sixteen %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
sseventeen <- seventeen %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
eeighteen <- eighteen %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
nnineteen <- nineteen %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
ttwentytwo <- twentytwo %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
ttwentythree <- twentythree %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
master <-
  bind_rows("2016" = ssixteen, "2017" = sseventeen, "2018" = eeighteen, "2019" = nnineteen, "2022" = ttwentytwo,  "2023" = ttwentythree, .id = "Year")

master_tempo <- ggplot(master, aes(x = Year, y = avg_temp, size = avg_danceability, label = avg_temp)) +
  geom_point() +
  ylim(120, 125) +
  labs(y = "Average Tempo", size = "Average Danceability", title = "Coachella Danceability and Tempo Trends") +
  theme(legend.position="bottom") +
  theme_minimal()

ggplotly(master_tempo)
```

# BeyoncÃ© 2019 {.storyboard}

### "Formation" Live Chromagram

```{r beyonce_live}
beyonce_live <-
  readRDS(file = "data/beyonce_live.RDS")

beyonce_livegraph <-
  readRDS(file = "data/beyonce_livegraph.RDS")

ggplotly(beyonce_livegraph)
```

------------------------------------------------------------------------

An important part of Coachella history (and history in general) is BeyoncÃ©'s "Homecoming" live album, which is her headline performance from Coachella 2019. This epic show was almost two hours long, and had the production quality of a Super Bowl halftime show. A Netflix documentary was made to immortalize the performance, something that is extremely rare for Coachella artists. Here, we will look at chromagrams for one of the songs she performed, "Formation", and use dynamic time warping to observe how the life performance differs from the album version.

### "Formation" Recording Chromagram

```{r beyonce_recording}
beyonce_recording <-
  readRDS(file = "data/beyonce_recording.RDS")

recording_graph <- 
  readRDS(file = "data/recording_graph.RDS")

ggplotly(recording_graph)
```

------------------------------------------------------------------------

Here we see a chromagram, which, at any given time window in the music, tells us how much of each pitch we are hearing relative to the others. Looking at this chromagram, for example, we can see that for most of the song we are getting strong values from the F key. We can attribute that to the constant wobbly electronic tone. Interestingly, we see almost a full scale in the first 20-30 seconds. I'm not exactly sure why this happens --- there is a two note echoing sound playing and her voice a cappella. With the voice having such a broad range of harmonics, this may explain the multiple notes, but not the ascending scale. There are low drone notes that rise, but over shorter periods of time. The ascending here looks like a very clean, distinct scale.

### Dynamic Time Warping

```{r yoncedtw}
yoncedytw <- 
  readRDS(file = "data/yoncedytw.RDS")

yoncedytw
```

------------------------------------------------------------------------

Unfortunately, the DTW is too large of a computation to render in `ggplotly`, so an image is included in its place.

Proper alignment in a DTW visualization will look like a diagonal line, which we can very, very faintly see at certain time windows in the graph. This means that at some points in the performance, it was very well matched up to the original recording, and vice versa. However, with the differences in timing between the two performances, it may be hard to accurately compare them. The dynamic time warping at each point can be thought of as a cost function regarding how costly it would be to make some sort of edit to align the tracks perfectly --- a similar problem to the [edit distance problem.](https://en.wikipedia.org/wiki/Edit_distance){target="_blank"}. A dynamic programming approach is used in both sequence-aligning problems.

# Frank vs. BTS {.storyboard}

### Cepstrograms

```{r btschroma, echo=FALSE}
bts <-
  readRDS(file = "data/bts.RDS")

btscepstro <- 
  readRDS(file = "data/btscepstro.RDS")

frank <-
  readRDS(file = "data/frank.RDS")


frankcepstro <- 
  readRDS(file = "data/frankcepstro.RDS")

cepstros <- 
  subplot(ggplotly(btscepstro), ggplotly(frankcepstro))
  
annotations = list(
  list(x = 0.25, y = 1.0, text = "Butter by BTS", xref = "paper", yref = "paper", xanchor = "center", yanchor = "bottom", showarrow = FALSE),
  list(x = 0.75, y = 1.0, text = "Futura Free by Frank Ocean", xref = "paper", yref = "paper", xanchor = "center", yanchor = "bottom", showarrow = FALSE))

cepstros <- cepstros %>%
  layout(annotations = annotations)
cepstros
```

------------------------------------------------------------------------

In order to further explore the range of Coachella's headliners, we may want to explore how their music differs. To analyze song structure, instrumentation, and more, we can conduct a chroma and timbre analysis of songs by two of the three 2023 headliners. Here is an analysis of "Futura Free" by Frank Ocean, and "Butter" by BTS, through the lens of cepstrograms and self-similarity matrices.

Here in the cepstrograms, we can see how Butter by BTS is split into about 5 sections. The intro has some different elements than the rest of the song, and we can discover upon listening that the intro is the one part in the song that has very little instrumental, mainly just voice and drums. Once the instrumental comes in, the first few MFCC's become more present.

In the cepstrogram of Futura Free by Frank Ocean, there is a lot going on, but maybe most interesting is around 280-315 seconds, when almost all the MFCC's are below 0.5 except C02. Upon listening, this is around 2:38 when the instrumental swells up, there is distorted noise, then we hear a focus on Frank's voice over very light, sparse piano chords. The section from 315 onwards is an audio collage of different interviews and mainly speaking.

### "Butter" Self-Similarity Matrices

```{r, echo=FALSE}
bts_ssm <-
  readRDS(file = "data/bts_ssm.RDS")

ggplotly(bts_ssm)
```

------------------------------------------------------------------------

Now onto the self-similarity matrices. The purpose of comparing these two headliners is to show that the headliners encompass a range of genres. BTS is more standard pop music, with most of their music sounding formulaic in the genre of pop. Frank Ocean is more experimental and complex in his instrumentals. Using a self-similarity matrix is a great way to compare these overall complexities.

For BTS, we see a distinct checkerboard pattern. Here, we can use that to say there is a lot of homogeneity in the song. There are blocks in the song to represent different sections, but overall there is a lot of similarity between early and late sections in the song, chroma-wise. In the timbre section, the brighter crosses represent novelty, or something new in the song. We don't see much of this, and the timbral analysis shows that most of the song has fairly similar instrumentation, apart from the initial 25 or so seconds.

### "Futura Free" Self-Similarity Matrices

```{r, echo=FALSE}
frank_ssm <-
  readRDS(file = "data/frank_ssm.RDS")

ggplotly(frank_ssm)
```

------------------------------------------------------------------------

For Frank Ocean, we see a lot more complexity. In this case, this is shown with a lot of dark blue and little yellow --- the magnitude of similarity at many points in the song is low, meaning there are few repeated sections or little homogeneity in the track. The timbral analysis shows a similar insight, with a novel musical moment around 315 seconds (this is the aforementioned speaking collage). Listening is always important because while this may signal to us that there will be some crazy different instrumental section at this point, it is in fact more visible because it is only vocals, which happens only at this section. Instead of an "interesting" musical moment, you could describe this moment as an "uninteresting" moment among an otherwise interesting song timbre-wise, so it stands out in the self-similarity matrix.

# EDM, Melody, and Tempo {.storyboard}

### Melody: A Chordogram

```{r, fig.width=5, fig.height=5}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(5.0, 2.0, 3.5, 2.0, 4.5, 4.0, 2.0, 4.5, 2.0, 3.5, 1.5, 4.0)
minor_key <-
  c(5.0, 2.0, 3.5, 4.5, 2.0, 4.0, 2.0, 4.5, 3.5, 2.0, 1.5, 4.0)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

# prydz <-
#   get_tidy_audio_analysis("3v2oAQomhOcYCPPHafS3KV?si=5728aa7d80bc4a4a") |>
#   compmus_align(sections, segments) |>
#   select(sections) |>
#   unnest(sections) |>
#   mutate(
#     pitches =
#       map(segments,
#           compmus_summarise, pitches,
#           method = "rms", norm = "euclidean"
#       )
#   )

prydz_graph <-
  readRDS(file = "data/prydz_graph.RDS")


ggplotly(prydz_graph)
```

------------------------------------------------------------------------

Coachella is full of electronic music. Electronic music as a genre can range from highly melodic complex tonal music to very limited techno beats that may only really sound like drums and bass. I would like to analyze Opus by Eric Prydz, a 9 minute masterpiece with an abnormally long buildup, and see how the chordogram recognizes the tonal changes, if any. We see the song start out hovering around A major and F# minor, which is classified to be F# minor by [Chordify](https://chordify.net/){target="_blank"}. We then see something interesting ---Â at about 120 seconds, we see the tonal center start to become less pronounced. The clear F# minor is no longer so clear. This may be because the drums start coming in, which continue for the rest of the song. Drums can add a lot of harmonics that would throw off the pitch values. We can still see a center around that F#min block, but there is also activity in the Gb7, and the Eb7 regions. It should be noted that these values were computed using [David Temperley's key profiles](http://davidtemperley.com/wp-content/uploads/2015/11/temperley-mp99.pdf){target="_blank"}, a reconsideration of the classic Krumhansl-Schmuckler Key-Finding Algorithm.

### Tempo: A... Tempogram

```{r}
prydz_tempo <-
  readRDS(file = "data/prydz_tempo.RDS")

ggplotly(prydz_tempo)
```

------------------------------------------------------------------------

If the last slide didn't inspire you to pull up the track and hear the changes and follow along, maybe this one will. Not only does Eric Prydz have a very interesting melodic visualization, but when we visualize the tempo of the song using a Fourier based, cyclic tempogram, we get a pretty stunning result. We see here how the song is analyzed to start out around the 80bpm mark or below, and slowly starts ramping up. In fact, it's a build up that starts at the beginning of the song and continues until around 3 minutes and 10 seconds where it settles on its main tempo of about 126 beats per minute. The extra lines that we see above and to the left are what is called tempo harmonics --- listening to the song, we can confirm the true tempo line is the main one we see in the middle, but if we were to zoom out on this graph we would see the horizontal line repeated around 252 beats per minute, exactly double of where we see it on this graph. This is because the software is unsure whether to count the beat for its value, or double, or half, or another way. We also see the tempo fade back down starting at around 400 seconds. There is a bit of a breakdown at 330 seconds, or 5 minutes and 30 seconds, which is audible but in my opinion is very unnoticeable compared to how pronounced the drop is on the tempogram.

# Headliner Clustering {data-orientation="columns"}

## Column {data-width="200"}

### **Commentary**

Coachella has had a wide array of headliners over the past six years, ranging over diverse genres such as KPOP, rap, psychedelic rock, classic rock, R&B, and more. While each of these headliners earned their spot by being a legend and paving their own way, we can still do some grouping amongst them. The dendrogram to the right is a visualization of a clustering technique applied to [a playlist I created](https://open.spotify.com/playlist/7uPCMiwKhG0SSHO9Ivx5FL?si=a7a9311545894655){target="_blank"}, with the artist names on the right and their #1 song, which was used for clustering, on the left. Of course, this isn't the best method, since perhaps the artist's #1 song is not very representative of their style, but it should be good enough to get some general groupings.

The clustering was calculated based on based on pitches, timbre, danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration and key features of the tracks, and the dendrogram was created using complete linkage. Looking on the lowest level, we can perhaps start to make out some subgenres and make sense of the clusters.

â€¢ ***The Less I Know The Better*** and ***Sweet Child O' Mine*** were grouped together, but interestingly enough (both being rock songs) they were separated pretty distinctly from ***Creep*** by Radiohead. ***Sweet Child O' Mine*** is classic rock at its finest, and ***TLIKTB*** by Tame Impala is in a similar world, in psychedelic rock, so this grouping makes sense.

â€¢ Really interestingly, perhaps by coincidence, both of The Weeknds songs in this corpus ended up perfectly together. This is interesting because ***Moth To A Flame*** is a Swedish House Mafia song with a Weeknd feature. Perhaps his style is influential enough on that track. This is where my assumption might hurt this visualization --- we can't take this to mean that Swedish House Mafia and The Weeknd should always be grouped together.

â€¢ ***Redbone***, ***7 rings***, and ***lovely*** are, in my opinion, the most radio/mainstream pop songs on the playlist, and they ended up together. Perhaps the 4 chord melody and standard rhythm patterns signaled that to our model.

## Column {data-width="800"}

### **Dendrogram**

```{r}
headliners_dendro <- readRDS(file = "data/headliners_dendro.RDS")

ggplotly(headliners_dendro)
```

# Discussion {data-orientation="columns"}

## Column {data-width="800"}

### **Commentary**

Through exploring very broad to very specific aspects of Coachella lineups over the past decade, we have explored how different genres, styles, artists, and songs make up the festival as we know it.

In the overview section, we were able to make broad generalizations and analyses about the 2023 lineup compared to the 2022, on basis of loudness, danceability, energy, and mode.

Then we dove deeper into danceability with a section where we analyzed the average tempo and danceability of each Coachella lineup from 2016-2023, finding that, on average, danceability is increasing which aligns with a steady rise in EDM at the festival.

With Beyonce, we analyzed one of the most iconic performances through the use of a chromagram which visualized her melodic powers in both her live and studio recordings of the song "Formation", and showed through a dynamic time warping how these versions were similar and different in their chroma features.

Then, to show range of headliners, we compared and contrasts musical structure through cepstrograms and chroma and timbre analysis between pop band BTS and neo soul artist Frank Ocean. Here we saw how a typical radio pop song shows a very structured self-similarity matrix, while a song with more musicality and instrumentation will have less of a stark pattern.

On the topic of genre, we moved to EDM, a popular genre at the festival as discussed, to show how an EDM song can play with our notions of key and tempo. Eric Prydz's "Opus" does just that and its unique attack on those features is visualized through a chordogram and tempogram.

Finally, we attempted to get some sort of grouping for the diverse madness that is the lineup by using a clustering algorithm based on pitches, timbre, danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration and key and saw how some of the headliners could be grouped together.

The festival, at the end of the day, can't be classified as one thing. The number of artists alone guarantees a baseline of diversity, but, as we've shown, the diversity is certainly not at a minimum. There are artists with pure EDM playing stages away from radio pop. R&B and experimental slow music competes with modern hip-hop and rap.

To Coachella organizers at Goldenvoice, analysis like this could be important in deciding the future of their festival. It's important to quantify and reflect on these types of things when faced with criticism as shown in the introduction. That being said, I think they're doing an okay job.

## Column {data-width="200"}

### **References**

â€¢ All of the data collected for this project was collected from Spotify using the official Spotify API.

â€¢ The visualizations for this project were made with the help of Dr. John Ashley Burgoyne and his teaching staff.

â€¢ The tools and libraries used in the making of this project are:

-   [`compmus`](https://github.com/jaburgoyne/compmus) (by Dr. Burgoyne) for complex computational musicology functions

-    [`spotifyr`](https://www.rcharlie.com/spotifyr/) for interacting with the Spotify API with R

-   [`ggplot2`](https://ggplot2.tidyverse.org/) for plotting data, and making it interactive

-    [`ggthemes`](https://cran.r-project.org/web/packages/ggthemes/index.html) for theming plots

-   [`ggdendro`](https://cran.r-project.org/web/packages/ggdendro/index.html) , for creating and rendering the dendrogram

-   [`flexdashboard`](https://pkgs.rstudio.com/flexdashboard/) , for the interactive dashboard and site setup

â€¢ The playlists were made by Spotify user [Alex Rodriguez](https://open.spotify.com/user/aarod333){target="_blank"} and myself.

â€¢ This project was completed at the [Universiteit van Amsterdam](https://www.uva.nl/) in Spring 2023.
