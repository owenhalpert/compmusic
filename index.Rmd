---
title: "ðŸŒ´ Coachella Over The Years"
author: "Owen Halpert"
#date: "2023-02-14"
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: rows
    theme: lumen
    css: custom.css
    favicon: favicon.ico
---

```{r setup, echo=FALSE, results='hide', message=FALSE, warning=FALSE}
library(spotifyr)
library(ggplot2)
library(ggthemes)
library(tidyverse)
library(plotly)
library(compmus)
coachella2023 <- readRDS(file = "data/coachella2023.RDS")
coachella2022 <- readRDS(file = "data/coachella2022.RDS")
```

# Introduction

#### An online portfolio for Computational Musicology at the University of Amsterdam (Spring 2023) by Owen Halpert

I grew up with Coachella --- as a child I was surrounded by its influence in the Los Angeles region, in high school I was lucky enough to attend all four years, and in college I road tripped with my fellow students for the 2022 lineup.

I owe a lot of my passion for music and my broad taste in genres to attending Coachella. With an extremely diverse lineup, attendees can walk from one stage to the next and instantly be transported between wildly different musical experiences.

Even with this diversity, every year there are critics of the released Coachella lineup, as seen through the Reddit lineup announcement comments:

> Yea....this isn't the festival I went to in 2010ðŸ˜” -bigbrett666

> Awesome if you like pop and edm but sucks if you like live music. -dpcpv75

> This Coachella lineup is MID. No catering to the hip hop and rap fans. SMH debating selling my box -kmart2588

Some people say too much electronic, some say too little. Some say the artists are too new and unrecognizable, some complain about too many throwback artists and want more relevance.

While there is always some validity in each of these criticisms, for my project I'd like to further analyze the trends of Coachella artist selection. To do so, I will use musicological metrics to show variance in genre, danceability over the years, and changing levels of emotional "valence" (A Spotify measure from 0.0 to 1.0 describing the "musical positiveness" conveyed by a track). I will look at artists' live performances at Coachella, analyze melodic aspects of different tracks, and more.

The Spotify tracks I will use to analyze this progression will be yearly lineup playlists. I can then analyze and compare years as a whole. These playlists were assembled by user [Alex Rodriguez](https://open.spotify.com/user/aarod333){target="_blank"}, who explained that the playlists are made up of "Top 5 songs per artist, attempt to spread across their notable releases and shorter edits where possible."

Shortcomings with this methodology might be that the selected tracks for each artist may not be representative of their true genre, that the playlists may not always be fully inclusive of all artists on the lineup, or that they are simply so large that meaningful comparisons by some metrics will be difficult to make. Nonetheless, we can use these as a starting point to focus in on some trends within instead of trying to analyze the years as a whole.

To give a sense of the breadth of these playlists, we can examine the outliers. Looking at the [2023 lineup playlist](https://open.spotify.com/playlist/2HfbF1Xx4RHhJI8jNvQAQb){target="_blank"}, an extremely atypical track is BTSTU by Jai Paul. Paul was added to the lineup as an artist with a cult following who is extremely secretive and has never performed live. He is as elusive, if not more, than headliner Frank Ocean. Conversely, we see One Kiss by Calvin Harris, which we can say is much more typical, given that Harris has performed at Coachella 8 times and is one of the most mainstream EDM artists.

# Recent Year Overviews {data-orientation="columns"}

## Column {data-width="200"}

### Commentary

The below charts give a high-level broad overview of major audio features for tracks in the [2022](https://open.spotify.com/playlist/2tcwxeUSNT7gYMzidaf5W1){target="_blank"} and [2023](https://open.spotify.com/playlist/2HfbF1Xx4RHhJI8jNvQAQb){target="_blank"} playlists. Specifically, captured are the valence, energy, danceability (size of dots, mouse over for values) and mode. With the size of these datasets it is difficult to make broad comparisons, but some observations could be as follows:

â€¢ While 2022 shows most of the high valence songs having a major key, 2023 is more of a mix, with some of the highest valence songs being in a minor key.

â€¢ There are slightly fewer high valence, low energy tracks in 2022 than there are in 2023.

â€¢ Similar to the first point, upon isolating the minor key tracks (this can be done by clicking the legend) they seem to be on average higher energy and valence than the minor key tracks in 2022.

In the next page, we will further examine danceability.

```{r mutate, echo=FALSE}
coachella20232 <- coachella2023 %>%               
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  )
coachella20222 <- coachella2022 %>%               
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  )
```

## Column {.tabset data-width="800"}

### 2022

```{r graph1, echo=FALSE}
plot1 <- ggplot(coachella20222, 
                aes(x = valence,
                    y = energy,
                    color = mode,
                    size = danceability)) +
  geom_point(position = "jitter", alpha = 0.4) +
  labs(x = "Valence", y = "Energy", color = "Mode", size = "", title = "Coachella 2022") +
  theme(legend.position="bottom") +
  theme_minimal()

ggplotly(plot1)
```

### 2023

```{r graph2}
plot2 <- ggplot(coachella20232, aes(x = valence, y = energy, size = danceability, color = mode)) +
  geom_point(position = "jitter", alpha = 0.4) +
  labs(x = "Valence", y = "Energy", color = "Mode", size = "", title = "Coachella 2023") +
  theme_minimal()
  
ggplotly(plot2)
```

# Deeper into Danceability

### Commentary {data-width="200"}

Over the past seven years, with the rise of EDM in pop culture and in Coachella's lineups, one may ask if the festival is becoming more dancey. What we see below is the average tempi and danceability scores (a Spotify quantification) from each year. This is no easy task computationally speaking, in fact, the graph below involves over 6,000 data points, around 1000 songs curated from each year's lineup by [Alex Rodriguez](https://open.spotify.com/user/aarod333?si=271917dd12624d62){target="_blank"}.

We can see the average tempo mostly decreasing, and the average danceability mostly increasing. This is interesting, because normally you would assume these metrics go hand in hand. In reality, Spotify doesn't explicitly give their calculation methodology for Danceability. They simply say,

"Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable."

So, if Danceability were to increase with tempo decreasing, maybe there has been a rise in these other categories of rhythm stability, beat strength, and overall regularity. If you look at [this graphic](https://www.billboard.com/wp-content/uploads/media/history-of-coachella-acts-by-genre-2015-billboard-graphic-510.jpg){target="_blank"} by Billboard, you can see a sharp increase in house/dance music and steady prevalence of electronic, which may explain the uptick in danceability.

### Danceability {data-width="800"}

```{r}
sixteen <- 
  readRDS(file = "data/sixteen.RDS")

seventeen <- 
  readRDS(file = "data/seventeen.RDS")

eighteen <- 
  readRDS(file = "data/eighteen.RDS")

nineteen <- 
  readRDS(file = "data/nineteen.RDS")

twentytwo <- 
  readRDS(file = "data/twentytwo.RDS")

twentythree <-
  readRDS(file = "data/twentythree.RDS")

ssixteen <- sixteen %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
sseventeen <- seventeen %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
eeighteen <- eighteen %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
nnineteen <- nineteen %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
ttwentytwo <- twentytwo %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
ttwentythree <- twentythree %>%
  summarize(avg_temp = mean(tempo), avg_danceability = mean(danceability))
master <-
  bind_rows("2016" = ssixteen, "2017" = sseventeen, "2018" = eeighteen, "2019" = nnineteen, "2022" = ttwentytwo,  "2023" = ttwentythree, .id = "Year")

master_tempo <- ggplot(master, aes(x = Year, y = avg_temp, size = avg_danceability, label = avg_temp)) +
  geom_point() +
  ylim(120, 125) +
  labs(y = "Average Tempo", size = "Average Danceability", title = "Coachella Danceability and Tempo Trends") +
  theme(legend.position="bottom") +
  theme_minimal()

ggplotly(master_tempo)
```

# BeyoncÃ© 2019 {.storyboard}

### "Formation" Live Chromagram

```{r beyonce_live}
beyonce_live <-
  readRDS(file = "data/beyonce_live.RDS")

beyonce_livegraph <-
  readRDS(file = "data/beyonce_livegraph.RDS")

ggplotly(beyonce_livegraph)
```

------------------------------------------------------------------------

An important part of Coachella history (and history in general) is BeyoncÃ©'s "Homecoming" live album, which is her headline performance from Coachella 2019. This epic show was almost two hours long, and had the production quality of a Super Bowl halftime show. A Netflix documentary was made to immortalize the performance, something that is extremely rare for Coachella artists. Here, we will look at chromagrams for one of the songs she performed, "Formation", and use dynamic time warping to observe how the life performance differs from the album version.

### "Formation" Recording Chromagram

```{r beyonce_recording}
beyonce_recording <-
  readRDS(file = "data/beyonce_recording.RDS")

recording_graph <- 
  readRDS(file = "data/recording_graph.RDS")

ggplotly(recording_graph)
```

------------------------------------------------------------------------

TODO: ADD ANALYSIS

### Dynamic Time Warping

```{r yoncedtw}
yoncedytw <- 
  readRDS(file = "data/yoncedytw.RDS")

yoncedytw
```

------------------------------------------------------------------------

Unfortunately, the DTW is too large of a computation to render in ggplotly, so an image is included in its place.

Proper alignment in a DTW visualization will look like a diagonal line, which we can very, very faintly see at certain time windows in the graph. This means that at some points in the performance, it was very well matched up to the original recording, and vice versa. However, with the differences in timing between the two performances, it may be hard to accurately compare them.

# Frank vs. BTS {.storyboard}

### Cepstrograms

```{r btschroma, echo=FALSE}
bts <-
  readRDS(file = "data/bts.RDS")

btscepstro <- 
  readRDS(file = "data/btscepstro.RDS")

frank <-
  readRDS(file = "data/frank.RDS")


frankcepstro <- 
  readRDS(file = "data/frankcepstro.RDS")

cepstros <- 
  subplot(ggplotly(btscepstro), ggplotly(frankcepstro))
  
annotations = list(
  list(x = 0.25, y = 1.0, text = "Butter by BTS", xref = "paper", yref = "paper", xanchor = "center", yanchor = "bottom", showarrow = FALSE),
  list(x = 0.75, y = 1.0, text = "Futura Free by Frank Ocean", xref = "paper", yref = "paper", xanchor = "center", yanchor = "bottom", showarrow = FALSE))

cepstros <- cepstros %>%
  layout(annotations = annotations)
cepstros
```

------------------------------------------------------------------------

For this week's homework we add a chroma and timbre analysis of songs by two of the three 2023 headliners to see how they may differ. Here is an analysis of "Futura Free" by Frank Ocean, and "Butter" by BTS, through the lens of cepstrograms and self-similarity matrices.

Here in the cepstrograms, we can see how Butter by BTS is split into about 5 sections. The intro has some different elements than the rest of the song, and we can discover upon listening that the intro is the one part in the song that has very little instrumental, mainly just voice and drums. Once the instrumental comes in, the first few MFCC's become more present.

In the cepstrogram of Futura Free by Frank Ocean, there is a lot going on, but maybe most interesting is around 280-315 seconds, when almost all the MFCC's are below 0.5 except C02. Upon listening, this is around 2:38 when the instrumental swells up, there is distorted noise, then we hear a focus on Frank's voice over very light, sparse piano chords. The section from 315 onwards is an audio collage of different interviews and mainly speaking.

### "Butter" Self-Similarity Matrices

```{r, echo=FALSE}
bts_ssm <-
  readRDS(file = "data/bts_ssm.RDS")

ggplotly(bts_ssm)
```

------------------------------------------------------------------------

Now onto the self-similarity matrices. The purpose of comparing these two headliners is to show that the headliners encompass a range of genres. BTS is more standard pop music, with most of their music sounding formulaic in the genre of pop. Frank Ocean is more experimental and complex in his instrumentals. Using a self-similarity matrix is a great way to compare these overall complexities.

For BTS, we see a distinct checkerboard pattern. Here, we can use that to say there is a lot of homogeneity in the song. There are blocks in the song to represent different sections, but overall there is a lot of similarity between early and late sections in the song, chroma-wise. In the timbre section, the brighter crosses represent novelty, or something new in the song. We don't see much of this, and the timbral analysis shows that most of the song has fairly similar instrumentation, apart from the initial 25 or so seconds.

### "Futura Free" Self-Similarity Matrices

```{r, echo=FALSE}
frank_ssm <-
  readRDS(file = "data/frank_ssm.RDS")

ggplotly(frank_ssm)
```

------------------------------------------------------------------------

For Frank Ocean, we see a lot more complexity. In this case, this is shown with a lot of dark blue and little yellow --- the magnitude of similarity at many points in the song is low, meaning there are few repeated sections or little homogeneity in the track. The timbral analysis shows a similar insight, with a novel musical moment around 315 seconds (this is the aforementioned speaking collage). Listening is always important because while this may signal to us that there will be some crazy different instrumental section at this point, it is in fact more visible because it is only vocals, which happens only at this section. Instead of an "interesting" musical moment, you could describe this moment as an "uninteresting" moment among an otherwise interesting song timbre-wise, so it stands out in the self-similarity matrix.

# EDM and Melody

### Chordogram {data-width="800"}

```{r, fig.width=5, fig.height=5}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(5.0, 2.0, 3.5, 2.0, 4.5, 4.0, 2.0, 4.5, 2.0, 3.5, 1.5, 4.0)
minor_key <-
  c(5.0, 2.0, 3.5, 4.5, 2.0, 4.0, 2.0, 4.5, 3.5, 2.0, 1.5, 4.0)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

# prydz <-
#   get_tidy_audio_analysis("3v2oAQomhOcYCPPHafS3KV?si=5728aa7d80bc4a4a") |>
#   compmus_align(sections, segments) |>
#   select(sections) |>
#   unnest(sections) |>
#   mutate(
#     pitches =
#       map(segments,
#           compmus_summarise, pitches,
#           method = "rms", norm = "euclidean"
#       )
#   )

prydz_graph <-
  readRDS(file = "data/prydz_graph.RDS")


ggplotly(prydz_graph)
```

### Commentary {data-width="200"}

Coachella is full of electronic music. Electronic music as a genre can range from highly melodic complex tonal music to very limited techno beats that may only really sound like drums and bass. I would like to analyze Opus by Eric Prydz, a 9 minute masterpiece with an abnormally long buildup, and see how the chordogram recognizes the tonal changes, if any. We see the song start out hovering around A major and F# minor, which is classified to be F# minor by [Chordify](https://chordify.net/){target="_blank"}. We then see something interesting ---Â at about 120 seconds, we see the tonal center start to become less pronounced. The clear F# minor is no longer so clear. This may be because the drums start coming in, which continue for the rest of the song. Drums can add a lot of harmonics that would throw off the pitch values. We can still see a center around that F#min block, but there is also activity in the Gb7, and the Eb7 regions. It should be noted that these values were computed using [David Temperley's key profiles](http://davidtemperley.com/wp-content/uploads/2015/11/temperley-mp99.pdf){target="_blank"}, a reconsideration of the classic Krumhansl-Schmuckler Key-Finding Algorithm.

# Discussion

Through exploring different areas, from very broad to very specific, of Coachella lineups over the past decade, we have explored how different
